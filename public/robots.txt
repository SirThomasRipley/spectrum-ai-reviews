# Robots.txt for SpectrumAIReviews.com
# Optimized for maximum SEO performance and crawl efficiency

# ============================================
# GLOBAL DIRECTIVES (All Search Engines)
# ============================================
User-agent: *

# Allow all content pages
Allow: /
Allow: /reviews/
Allow: /ai-writing-tool-reviews
Allow: /ai-art-generator-reviews
Allow: /ai-seo-tool-reviews
Allow: /ai-assistant-agent-reviews
Allow: /about
Allow: /methodology
Allow: /affiliate-disclosure

# Block technical and admin paths ONLY
Disallow: /api/
Disallow: /_next/
Disallow: /admin/

# Allow CSS and JS for proper rendering (important for Google)
Allow: /*.css
Allow: /*.js

# ✅ REMOVED parameter blocking (lines 24-32) to prevent soft 404s
# Note: Canonical tags handle duplicate content better than robots.txt blocking
# Blocking URL parameters can cause soft 404s if Google has indexed URLs with these params

# ============================================
# GOOGLEBOT SPECIFIC
# ============================================
User-agent: Googlebot
Allow: /
Crawl-delay: 0
Request-rate: 1/1

# Allow Googlebot to crawl all images
User-agent: Googlebot-Image
Allow: /images/
Allow: /*.jpg
Allow: /*.jpeg
Allow: /*.png
Allow: /*.gif
Allow: /*.webp
Allow: /*.svg

# ============================================
# BINGBOT SPECIFIC
# ============================================
User-agent: Bingbot
Allow: /
Crawl-delay: 0
Request-rate: 1/1

# ============================================
# OTHER MAJOR SEARCH ENGINES
# ============================================

# Yahoo (Slurp)
User-agent: Slurp
Allow: /
Crawl-delay: 1

# DuckDuckGo
User-agent: DuckDuckBot
Allow: /
Crawl-delay: 0

# Yandex
User-agent: Yandex
Allow: /
Crawl-delay: 1

# Baidu (Chinese search engine)
User-agent: Baiduspider
Allow: /
Crawl-delay: 1

# ============================================
# AI CRAWLERS (For LLM Training & AI Search)
# ============================================

# OpenAI (ChatGPT)
User-agent: GPTBot
Allow: /
Crawl-delay: 1

# Google Bard/Gemini
User-agent: Google-Extended
Allow: /
Crawl-delay: 1

# Anthropic Claude
User-agent: ClaudeBot
Allow: /
Crawl-delay: 0

# Common Crawl (Web Archive)
User-agent: CCBot
Allow: /
Crawl-delay: 2

# ============================================
# BLOCK BAD BOTS & SCRAPERS
# ============================================

# ✅ FIXED: Throttle instead of block for SEO tool bots (important for backlink signals)
User-agent: AhrefsBot
Crawl-delay: 5

User-agent: SemrushBot
Crawl-delay: 5

User-agent: MJ12bot
Crawl-delay: 5

User-agent: DotBot
Crawl-delay: 5

# Block content scrapers
User-agent: SurveyBot
Disallow: /

User-agent: MauiBot
Disallow: /

User-agent: BLEXBot
Disallow: /

# ============================================
# SITEMAP LOCATIONS
# ============================================
Sitemap: https://spectrumaireviews.com/sitemap.xml

# Host directive (for search engines that support it)
Host: https://spectrumaireviews.com